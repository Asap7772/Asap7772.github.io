<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anikait Singh</title>

    <meta name="author" content="Anikait Singh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anikait Singh
                </p>
                <p>I'm a Ph.D Student at Stanford AI and a student researcher at Google DeepMind Robotics, based in Mountain View. My research is supported by the NSF Graduate Research Fellowship.
                </p>
                <p>
                  Previously, I was at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, and <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a> as part of BAIR working on Deep RL and Robot Learning.
                </p>
                <p style="text-align:center">
                  <a href="mailto:anikait@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="anikait_data/Resume_Anikait_Singh.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lPaISmIAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Anikait_Singh_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/asap7772/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="anikait_images/anikaitsingh.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="anikait_images/anikaitsingh.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My primary research interests are in decision-making methods such as reinforcement learning and scaling them up. I believe that a good target of my research would be to produce foundation models for decision-making that utilize large diverse data sources that show good generalization and enable rapid learning.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/workflow.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2109.10813.pdf" id="workflow_arxiv">
                  <span class="papertitle">A Workflow for Offline Model-Free Robotic Reinforcement Learning</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh*</strong>, Stephen Tian, Chelsea Finn, Sergey Levine
                <br>
                <em>CoRL</em>, 2021, <em>(Oral Presentation)</em>
                <br>
                <a href="https://sites.google.com/view/offline-rl-workflow">project page</a> /
                <a href="https://arxiv.org/pdf/2109.10813.pdf">paper</a> /
                <a href="https://www.youtube.com/watch?v=h9R5LJX9b1I&list=PL2oxSfYMr6hXwFk4_rBL1xgASdp-aOkV9&index=4">talk</a>
                <p></p>
                <p>Our proposed workflow aims to detect overfitting and underfitting in model-free offline RL, and provides guidelines for addressing these issues via policy selection, regularization, and architecture design.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/ReDS.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2211.01052.pdf" id="workflow_arxiv">
                  <span class="papertitle">Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Aviral Kumar*, Quan Vuong, Yevgen Chebotar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2211.01052.pdf">paper</a>
                <p></p>
                <p>CQL (ReDS) modifies a typical distribution constraint into a support-level constraint via re-weighting to enable learning from Heteroskedastic Dataset Composotions.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/PTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2210.05178.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh</strong>*, Frederik Ebert*, Mitsuhiko Nakamoto
                <br>
                Yanlai Yang, Chelsea Finn, Sergey Levine
                <br>
                <em>RSS</em>, 2023
                <br>
                <a href="https://sites.google.com/view/ptr-final/">project page</a> /
                <a href="https://arxiv.org/pdf/2210.05178.pdf">paper</a> /
                <a href="https://www.youtubeeducation.com/watch?v=yAWgyLJD5lY">video</a>
                <p></p>
                <p>PTR is a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/VPTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.13041.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Robotic Offline RL from Internet Videos via Value-Function Pre-Training</span>
                </a>
                <br>
                Chethan Bhateja*, Derek Guo*, Dibya Ghosh*, <strong>Anikait Singh</strong>, Manan Tomar,
                <br>
                Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar
                <br>
                <a href="https://arxiv.org/pdf/2309.13041.pdf">paper</a>
                <p></p>
                <p>VPTR is a framework that combines the benefits of pre-training on video data with robotic offline RL approaches that train on diverse robot data, resulting in value functions and policies for manipulation tasks that are robust and generalizable.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/offlinerlvsbc.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?</span>
                </a>
                <br>
                Aviral Kumar, Joey Hong, <strong>Anikait Singh</strong>, Sergey Levine
                <br>
                <em>ICLR</em>, 2022
                <br>
                <a href="https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a>
                <p></p>
                <p>Theoretical paper that characterize the properties of environments that allow offline RL methods to perform better than BC methods, even when only provided with expert data. Additionally, policies trained on sufficiently noisy suboptimal data outperform BC algorithms with expert data, especially on long-horizon problems.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/CALQL.jpeg" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</span>
                </a>
                <br>
                Mitsuhiko Nakamoto*, Yuexiang Zhai*, <strong>Anikait Singh</strong>, Max Sobol Mark, 
                <br>
                Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://nakamotoo.github.io/Cal-QL/index.html">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a> /
                <a href="https://youtu.be/r9CCdLeMJTg">video</a> /
                <a href="https://github.com/nakamotoo/Cal-QL">code</a>
                <p></p>
                <p>A method that learns a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. This leads to effective online fine-tuning, enabling benefits of offline initializations in online fine-tuning</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/RT2.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf" id="workflow_arxiv">
                  <span class="papertitle"> RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</span>
                </a>
                <br>
                Google DeepMind Robotics
                <br>
                <em>ICRA</em>, 2023
                <br>
                <a href="https://robotics-transformer2.github.io/">project page</a> /
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/">blog</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/RTX.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2310.08864.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Open X-Embodiment: Robotic Learning Datasets and RT-X Models</span>
                </a>
                <br>
                Open X-Embodiment Collaboration
                <br>
                <em>CoRL</em>, 2023
                <br>
                <a href="https://robotics-transformer-x.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2310.08864.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">blog</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/schepen.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288" id="workflow_arxiv">
                  <span class="papertitle"> A Mobile Application for Keyword Search in Real-World Scenes</span>
                </a>
                <br>
                Shrinivas Pundlik, <strong>Anikait Singh</strong>, Gautam Baghel, Vilte Baliutaviciute, Gang Luo
                <br>
                IEEE Journal of Translational Engineering in Health and Medicine
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288">paper</a>
                <p></p>
                <p>System to help visually-impaired patients localize where words are present in a cluttered environment. This system utilizes OCR + Levenshtein Distance along with specialized audio cues and additional assistive features to enable efficient and intuitive search in crowded, diverse environments</p>
              </td>
            </tr>


          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="anikait_images/teaching.png" width="240" alt="csteaching">
              </td>
              <td width="75%" valign="center">
                <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">Undergraduate Student Instructor, CS285 Fall 2022</a>
                <br>
                <a href="https://inst.eecs.berkeley.edu/~cs188/sp22/">Undergraduate Student Instructor, CS188 Spring 2022</a>
                <br>
                <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">Undergraduate Student Instructor, CS285 Fall 2021</a>
              </td>
            </tr>

          </tbody></table>

        </td>
      </tr>
    </table>
  </body>

</html>
