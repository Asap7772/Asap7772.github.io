<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anikait Singh</title>

    <meta name="author" content="Anikait Singh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  </head>

  <body>
    <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anikait Singh
                </p>
                <p>
                  I'm a second year Ph.D Student at Stanford AI advised by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>. I closely collaborate with <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a> and <a href="https://thashim.github.io/">Tatsu Hashimoto</a>. Previously, I was at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> as part of BAIR working on Deep RL and Robot Learning. I was also a student researcher at Google DeepMind Robotics, and Toyota Research Institute. My research is supported by the NSF Graduate Research Fellowship.
                </p>
                <p style="text-align:center">
                  <a href="mailto:anikait@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="./static/pdf/resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lPaISmIAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Anikait_Singh_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/asap7772/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="./static/images/profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="./static/images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <hr>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research focuses on advancing decision-making methodologies, particularly in reinforcement learning, and scaling these approaches to broader applications. I am particularly interested in multi-step reasoning in complex domains such as code and mathematics, as well as designing alignment algorithms and personalization techniques. My ultimate goal is to develop foundational models for decision-making that leverage diverse, large-scale datasets to achieve robust generalization, efficient rapid learning, and adaptability to individual needs.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/FSPO.jpg" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2502.19312.pdf" id="understanding_rlhf_arxiv">
                  <span class="papertitle">FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Sheryl Hsu*, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn
                <br>
                <em>ACL</em>, 2025
                <br>
                <a href="https://fewshot-preference-optimization.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2502.19312.pdf">paper</a> /
                <a href="https://github.com/Asap7772/fewshot-preference-optimization">code</a>
                <p></p>
                <p>As language models increasingly interact with a diverse user base, it becomes important for models to generate responses that align with individual user preferences. Few-Shot Preference Optimization (FSPO) is a meta-learning framework that leverages the strong in-context learning capabilities of an LLM to capture the diversity of human preferences. We additionally study crucial design decision to allow for effective transfer from synthetic to real users through synthetic preference data.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/cog_behav.png" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2501.04682.pdf" id="understanding_rlhf_arxiv">
                  <span class="papertitle">Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs
                  </span>
                </a>
                <br>
                Kanishk Gandhi, Ayush Chakravarthy, <strong>Anikait Singh</strong>, Nathan Lile, Noah D. Goodman
                <br>
                <a href="https://arxiv.org/pdf/2501.04682.pdf">paper</a> /
                <a href="https://github.com/kanishkg/cognitive-behaviors">code</a> 
                <p></p>
                <p>This study demonstrates that a language model's intrinsic reasoning behaviors—such as verification, backtracking, subgoal setting, and backward chaining—are key to its ability to self-improve under reinforcement learning, as evidenced by Qwen naturally exhibiting these traits while Llama initially does not. By priming Llama with examples of proper reasoning patterns—even with incorrect answers—and further pretraining with targeted reasoning data, the model's performance substantially improves to match or exceed that of Qwen, underscoring the fundamental link between initial reasoning behaviors and effective model enhancement.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/metacot.png" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2501.04682.pdf" id="understanding_rlhf_arxiv">
                  <span class="papertitle">Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought
                  </span>
                </a>
                <br>
                Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, <strong>Anikait Singh</strong>, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, Chelsea Finn
                <br>
                <a href="https://arxiv.org/pdf/2501.04682.pdf">paper</a> /
                <a href="https://www.synthlabs.ai/research/meta-chain-of-thought">blog</a> 
                <p></p>
                <p>We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We
                  present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search,
                  and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search
                  algorithms.
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/understanding_rlhf.jpeg" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2404.14367.pdf" id="understanding_rlhf_arxiv">
                  <span class="papertitle">Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Fahim Tajwar*, Archit Sharma, Rafael Rafailov, 
                Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar
                <br>
                <em>ICML</em>, 2024
                <br>
                <a href="https://understanding-rlhf.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2404.14367.pdf">paper</a> /
                <a href="https://github.com/Asap7772/understanding-rlhf">code</a>
                <p></p>
                <p>Learning from preferences is a common paradigm for fine-tuning language models. 
                  Yet, many algorithmic design decisions come into play. Our new work finds that 
                  approaches employing on-policy sampling or negative gradients outperform offline, 
                  maximum likelihood objectives.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/d5rldomains.jpg" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=Aj1wftldeR" id="understanding_rlhf_arxiv">
                  <span class="papertitle">D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning</span>
                </a>
                <br>
                Rafael Rafailov*, Kyle Beltran Hatch*, <strong>Anikait Singh</strong>, Aviral Kumar, 
                Laura Smith, Ilya Kostrikov, Philippe Hansen-Estruch, Victor Kolev, 
                Philip J. Ball, Jiajun Wu, Sergey Levine, Chelsea Finn
                <br>
                <em>RCL</em>, 2024
                <br>
                <a href="https://sites.google.com/view/d5rl/">project page</a> /
                <a href="https://openreview.net/pdf?id=Aj1wftldeR">paper</a> /
                <a href="https://github.com/d5rlbenchmark/d5rl">code</a>
                <p></p>
                <p>Offline RL algorithms enable data-driven methods without the need for 
                  costly or dangerous real-world exploration, leveraging large pre-collected datasets. 
                  However, effective and challenging benchmarks that capture real-world task properties are 
                  necessary for evaluating progress, prompting the proposal of a new benchmark for offline RL 
                  based on realistic robotic simulations and diverse data sources to 
                  support both offline RL and online fine-tuning evaluation.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/VPTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.13041.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Robotic Offline RL from Internet Videos via Value-Function Pre-Training</span>
                </a>
                <br>
                Chethan Bhateja*, Derek Guo*, Dibya Ghosh*, <strong>Anikait Singh</strong>, Manan Tomar,
                <br>
                Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar
                <br>
                <em>ICRA</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2309.13041.pdf">paper</a> /
                <a href="https://dibyaghosh.com/vptr/">project page</a> /
                <a href="https://dibyaghosh.com/vptr/results.html">videos</a>
                <p></p>
                <p>
                  VPTR is a framework that combines the benefits of pre-training on video data with robotic offline RL approaches 
                  that train on diverse robot data, resulting in value functions and policies for manipulation tasks that are robust 
                  and generalizable.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/RTX.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2310.08864.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Open X-Embodiment: Robotic Learning Datasets and RT-X Models</span>
                </a>
                <br>
                Open X-Embodiment Collaboration
                <br>
                <em>CoRL</em>, 2024
                <br>
                <a href="https://robotics-transformer-x.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2310.08864.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">blog</a>
                <p>
                  This is an opensource dataset comprised of a large collection of robot embodiments. We study how vision-language 
                  models trained on X-Embodiment Datasets can enable efficient adaptation to new robots, tasks, and environments.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/RT2.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf" id="workflow_arxiv">
                  <span class="papertitle"> RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</span>
                </a>
                <br>
                Google DeepMind Robotics
                <br>
                <em>ICRA</em>, 2023
                <br>
                <a href="https://robotics-transformer2.github.io/">project page</a> /
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/">blog</a>
                <p></p>
                <p>
                  We study how vision-language models trained on Internet-scale data can be incorporated directly into 
                  end-to-end robotic control to boost generalization and enable emergent semantic reasoning.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/ReDS.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2211.01052.pdf" id="workflow_arxiv">
                  <span class="papertitle">Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Aviral Kumar*, Quan Vuong, Yevgen Chebotar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2211.01052.pdf">paper</a> /
                <a href="https://recorder-v3.slideslive.com/#/share?share=88978&s=bedd4cb9-6463-416b-91bf-80d276f07bf3">talk</a>
                <p></p>
                <p>
                  CQL (ReDS) is an offline RL method that modifies a typical distribution constraint into an approximate 
                  support-level constraint via re-weighting to enable efficient learning from heteroskedastic dataset compositions.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/CALQL.jpeg" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</span>
                </a>
                <br>
                Mitsuhiko Nakamoto*, Yuexiang Zhai*, <strong>Anikait Singh</strong>, Max Sobol Mark, 
                <br>
                Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://nakamotoo.github.io/Cal-QL/index.html">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a> /
                <a href="https://youtu.be/r9CCdLeMJTg">video</a> /
                <a href="https://github.com/nakamotoo/Cal-QL">code</a>
                <p></p>
                <p>
                  A method that learns a conservative value function initialization that underestimates the value of the 
                  learned policy from offline data, while also being calibrated, in the sense that the learned Q-values 
                  are at a reasonable scale. This leads to effective online fine-tuning, enabling benefits of offline 
                  initializations in online fine-tuning
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/PTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2210.05178.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Pre-Training for Robots: Offline RL Enables Learning from a Handful of Trials</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh</strong>*, Frederik Ebert*, Mitsuhiko Nakamoto
                <br>
                Yanlai Yang, Chelsea Finn, Sergey Levine
                <br>
                <em>RSS</em>, 2023
                <br>
                <a href="https://sites.google.com/view/ptr-final/">project page</a> /
                <a href="https://arxiv.org/pdf/2210.05178.pdf">paper</a> /
                <a href="https://www.youtubeeducation.com/watch?v=yAWgyLJD5lY">video</a>
                <p></p>
                <p>
                  PTR is a framework based on offline RL that attempts to effectively learn new tasks by combining 
                  pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 
                  10 demonstrations.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/offlinerlvsbc.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?</span>
                </a>
                <br>
                Aviral Kumar, Joey Hong, <strong>Anikait Singh</strong>, Sergey Levine
                <br>
                <em>ICLR</em>, 2022
                <br>
                <a href="https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a>
                <p></p>
                <p>
                  Theoretical paper that characterize the properties of environments that allow offline RL methods to 
                  perform better than BC methods, even when only provided with expert data. Additionally, policies trained 
                  on sufficiently noisy suboptimal data outperform BC algorithms with expert data, especially on long-horizon problems.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/workflow.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2109.10813.pdf" id="workflow_arxiv">
                  <span class="papertitle">A Workflow for Offline Model-Free Robotic Reinforcement Learning</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh*</strong>, Stephen Tian, Chelsea Finn, Sergey Levine
                <br>
                <em>CoRL</em>, 2021, <em>(Oral Presentation)</em>
                <br>
                <a href="https://sites.google.com/view/offline-rl-workflow">project page</a> /
                <a href="https://arxiv.org/pdf/2109.10813.pdf">paper</a> /
                <a href="https://www.youtube.com/watch?v=h9R5LJX9b1I&list=PL2oxSfYMr6hXwFk4_rBL1xgASdp-aOkV9&index=4">talk</a>
                <p></p>
                <p>
                  Our proposed workflow aims to detect overfitting and underfitting in model-free offline RL, 
                  and provides guidelines for addressing these issues via policy selection, regularization, and architecture design.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/schepen.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288" id="workflow_arxiv">
                  <span class="papertitle"> A Mobile Application for Keyword Search in Real-World Scenes</span>
                </a>
                <br>
                Shrinivas Pundlik, <strong>Anikait Singh</strong>, Gautam Baghel, Vilte Baliutaviciute, Gang Luo
                <br>
                IEEE Journal of Translational Engineering in Health and Medicine, 2019
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288">paper</a>
                <p></p>
                <p>
                  System to help visually-impaired patients localize where words are present in a cluttered environment. This system 
                  utilizes OCR + Levenshtein Distance along with specialized audio cues and additional assistive features to enable 
                  efficient and intuitive search in crowded, diverse environments.
                </p>
              </td>
            </tr>


          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <hr>
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/teaching.png" width="240" alt="csteaching">
              </td>
              <td width="75%" valign="center">
                Undergraduate Student Instructor, <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">CS285 Fall 2022</a>
                <br>
                Undergraduate Student Instructor, <a href="https://inst.eecs.berkeley.edu/~cs188/sp22/">CS188 Spring 2022</a>
                <br>
                Undergraduate Student Instructor, <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">CS285 Fall 2021</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/stanford_logo.png" width="240" alt="csteaching">
              </td>
              <td width="75%" valign="center">
                Program Coordinator, Mentor, <a href="https://deeplearningportal.org/">Deep Learning Portal 2024</a>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <hr>
            <tr>
              <td width="100%" valign="center" halign="center">
                <p style="text-align:center">
                  Website template by <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
    

  </body>

</html>
